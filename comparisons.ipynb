{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of different data hosting/downloading methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Write an overview, finish writing pros/cons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Unzipped v. Zipped \n",
    "\n",
    "In some of these examples, we discuss the case of storing the data directly vs storing the data as a zip. \n",
    "\n",
    "To avoid redundancy, we outline pros and cons of zipping the entire data set here:\n",
    "\n",
    "**Pros:**\n",
    "- In zenodo: we can preserve the nested data structure only if it's within a zip\n",
    "- Zenodo and OSF don't handle large amounts of files well\n",
    "\n",
    "**Cons:**\n",
    "- Versioning becomes harder; for example, adding a single filter file would require a reupload of the entire zip (currently 382MBs)\n",
    "\n",
    "**Notes:**\n",
    "- While not discussed in this notebook, but there is potential merit in *partially* zipping the data; for example, zipping the contents of sed/QSO/. This gives us some of the benefits of consolidated data, while providing us the flexibility to add and modify without needing to reupload everything again.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zenodo • *(No Demo)*\n",
    "\n",
    "**Pros:**\n",
    "- Version control, DOIs\n",
    "- No upper limit for storage capacity (but no files > 50GB per file)\n",
    "- Lifetime: same as CERN (at least 20 years)\n",
    "\n",
    "**Cons:**\n",
    "- CERN stores data in a way that doesn't really like many small files (find link for this)\n",
    "- Doesn't seem to support multiple contributors (I can list serveral authors to the dataset, but it's unclear if I can give other users permisison to edit the project)\n",
    "- Doesn't support nested directory structures, which is a no-go for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zenodo, with the data stored as a zip • [Project on Zenodo](https://zenodo.org/records/10843773)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  364M    0  364M    0     0  1290k      0 --:--:--  0:04:49 --:--:-- 1543k--  0:00:06 --:--:--  703k  0 --:--:--  0:00:35 --:--:--  766k--  777k0:52 --:--:--  765k--:--:--  0:00:56 --:--:--  760k--:--:--  0:01:00 --:--:--  765k--:--  771k2 --:--:--  775k01:08 --:--:--  779k 0   753k      0 --:--:--  0:01:09 --:--:--  788k 784k:--  0:01:11 --:--:--  778k   0 --:--:--  0:01:14 --:--:--  746k --:--:--  0:01:31 --:--:-- 1285k9k      0 --:--:--  0:02:41 --:--:-- 1366k 0 --:--:--  0:02:55 --:--:-- 1421k:03:47 --:--:-- 2159k:17 --:--:-- 2332k-  0:04:21 --:--:-- 1701k--:-- 1403k4:41 --:--:-- 1477k88k      0 --:--:--  0:04:47 --:--:-- 1480k\n"
     ]
    }
   ],
   "source": [
    "# Zenodo Zip Demo (~5 min)\n",
    "\n",
    "ZENODO_ZIP_DATA_URL=\"https://zenodo.org/api/records/10843773/files-archive\"\n",
    "! curl --output lephare_data_zenodo.zip $ZENODO_ZIP_DATA_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Science Framework (OSF) • [Project on OSF](https://osf.io/64eg9/)\n",
    "\n",
    "**Pros:**\n",
    "- Version control, DOIs\n",
    "- CLI tool [osfclient](https://pypi.org/project/osfclient/) for easier batch upload, among other functionality\n",
    "- Co-ownership: supports multiple contributors\n",
    "- Can edit and delete uploaded data\n",
    "- 50GB capacity for public projects\n",
    "- Lifetime: preservation fund for 50+ years after closing at current costs\n",
    "\n",
    "**Cons:**\n",
    "- Uploading nested subdirs via browser GUI (rather than `osfclient`) is tedious at best, semi-impossible at worst. This makes our `filt/` and `sed/` directories challenging.\n",
    "- osfclient is extremely slow when uploading a large set of files ([osfclient#155](https://github.com/osfclient/osfclient/issues/155), [#149](https://github.com/osfclient/osfclient/issues/149), [#146](https://github.com/osfclient/osfclient/issues/146); additionally, [nilearn#1925](https://github.com/nilearn/nilearn/issues/1925))\n",
    "- OSF is designed to store very few files, it cannot handle a large set of files ([osfclient#155](https://github.com/osfclient/osfclient/issues/155)).\n",
    "- Regional servers: I defaulted to the US (does it get more specific than this?) and naturally that's suboptimal for collaboration across Europe and US west coast. Options include: United States, Canada (Montréal), Germany (Frankfurt), Australia (Sydney)\n",
    "- (Potentially, though [this GitHub discussion is from 2018](https://github.com/osfclient/osfclient/issues/155#issuecomment-409196619), osfclient can miss some files in a batch upload)\n",
    "\n",
    "**Notes:**\n",
    "- Curling the link to the zip is not explicitly described in the official API docs, but is discussed as an option in GitHub issues ([osfclient#475](https://github.com/CenterForOpenScience/osf.io/issues/475))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  291M    0  291M    0     0   113k      0 --:--:--  0:43:50 --:--:-- 857271819      0 --:--:--  0:01:00 --:--:--     0:03:50 --:--:--     0 --:--:--  0:05:08 --:--:-- 12494   097      0 --:--:--  0:06:40 --:--:--     0-:--  0:06:53 --:--:-- 14438--:-- 11772--:--     0--:--     0  0 --:--:--  0:09:30 --:--:-- 1037k-:-- 12170 0:11:30 --:--:--     00  0 --:--:--  0:13:41 --:--:--     06 --:--:--     0 0 --:--:--  0:14:52 --:--:--     0--:--:--     015:12 --:--:--     065522      0 --:--:--  0:15:18 --:--:--     0-:--  0:15:41 --:--:--     0-:--:--     0 0 --:--:--  0:18:04 --:--:-- 14454:18:23 --:--:--     0 --:--:--  0:20:14 --:--:--     0--:--:--  0:20:16 --:--:--     0-:--:-- 13704--:--:-- 1345450 --:--:--  5675-  0:21:52 --:--:--     047398      0 --:--:--  0:22:17 --:--:-- 22606:22:30 --:--:--     01      0 --:--:--  0:22:46 --:--:-- 26447      0 --:--:--  0:23:44 --:--:--     03:54 --:--:--     0--  0:23:58 --:--:--     0--:--:--     0:--  0:25:15 --:--:-- 39290--:--:--  0:25:20 --:--:--  371k70      0 --:--:--  0:25:34 --:--:--  704k 0 --:--:--  0:25:35 --:--:--  702k37 --:--:--  642k     0 --:--:--  0:27:15 --:--:--  508k:27:36 --:--:--     0 0   102k      0 --:--:--  0:31:46 --:--:--  577k-:--  399k  0:32:36 --:--:--     0:19 --:--:-- 23264:21 --:--:-- 11774:--  0:33:25 --:--:-- 13437 --:--:--  0:33:28 --:--:--     030 --:--:-- 13633 0 --:--:--  0:34:00 --:--:--  632k:34:07 --:--:--  559k --:--:--  0:34:16 --:--:--  666k 0:34:36 --:--:--  637k0 --:--:--  0:37:00 --:--:--  155kk      0 --:--:--  0:37:22 --:--:-- 17766    0 --:--:--  0:38:16 --:--:-- 67772:-- 38870 --:--:--  0:38:48 --:--:-- 47723 --:--:--  0:40:25 --:--:-- 368938k      0 --:--:--  0:40:53 --:--:--  270k-:--:--  0:40:57 --:--:-- 40124- 288511:25 --:--:-- 25740--:--:-- 39959:10 --:--:-- 27108-:--:--  0:42:14 --:--:-- 62737      0 --:--:--  0:42:31 --:--:-- 34730-:-- 49620-  0:42:55 --:--:-- 36381 0\n"
     ]
    }
   ],
   "source": [
    "# OSF Demo (~43 mins)\n",
    "\n",
    "OSF_DATA_URL=\"https://files.osf.io/v1/resources/64eg9/providers/osfstorage/?zip=&_gl=1*1hf86d4*_ga*Nzk4MjQxNzMzLjE3MTAzNDQ3NzQ.*_ga_YE9BMGGWX8*MTcxMDUyOTI0OS41LjEuMTcxMDUyOTI2My40Ni4wLjA.\"\n",
    "! curl --output lephare_data_osf.zip \"$OSF_DATA_URL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSF, with the data stored as a zip • [Project on OSF](https://osf.io/mvpks/)\n",
    "\n",
    "**Notes:**\n",
    "- For this example, I did elect the European server (Germany) to see if it would make a large difference for me.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  364M    0  364M    0     0  26.6M      0 --:--:--  0:00:13 --:--:-- 28.6M13 --:--:-- 30.4M\n"
     ]
    }
   ],
   "source": [
    "# OSF as Zip Demo (~14 sec)\n",
    "\n",
    "OSF_AS_ZIP_DATA_URL=\"https://files.osf.io/v1/resources/mvpks/providers/osfstorage/?zip=&_gl=1*15ihtvy*_ga*Nzk4MjQxNzMzLjE3MTAzNDQ3NzQ.*_ga_YE9BMGGWX8*MTcxMDk0MjQzMi4xOC4xLjE3MTA5NDI3ODYuNTQuMC4w\"\n",
    "! curl --output lephare_data_osf_as_zip.zip \"$OSF_AS_ZIP_DATA_URL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Drive • [View File](https://drive.google.com/file/d/1gsni-hMPU5yGkJDyTcv5wV-vl9NCH7g0/view?usp=drive_link)\n",
    "\n",
    "**Pros:**\n",
    "- Reliable plan B for just getting data, especially for times of active development\n",
    "\n",
    "**Cons:**\n",
    "- I don't really see this as a long term solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /Users/orl/miniconda3/envs/scratch/lib/python3.10/site-packages (5.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/orl/miniconda3/envs/scratch/lib/python3.10/site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: filelock in /Users/orl/miniconda3/envs/scratch/lib/python3.10/site-packages (from gdown) (3.12.4)\n",
      "Requirement already satisfied: requests[socks] in /Users/orl/miniconda3/envs/scratch/lib/python3.10/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/orl/miniconda3/envs/scratch/lib/python3.10/site-packages (from gdown) (4.66.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/orl/miniconda3/envs/scratch/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/orl/miniconda3/envs/scratch/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/orl/miniconda3/envs/scratch/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/orl/miniconda3/envs/scratch/lib/python3.10/site-packages (from requests[socks]->gdown) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/orl/miniconda3/envs/scratch/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.7.22)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/orl/miniconda3/envs/scratch/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mDownloading...\n",
      "From (original): https://drive.google.com/uc?id=1gsni-hMPU5yGkJDyTcv5wV-vl9NCH7g0\n",
      "From (redirected): https://drive.google.com/uc?id=1gsni-hMPU5yGkJDyTcv5wV-vl9NCH7g0&confirm=t&uuid=936469b8-819d-4ef7-8f88-5eede1d7fc83\n",
      "To: /Users/orl/code/LEPHARE-demo/lephare_data_gdrive.zip\n",
      "100%|████████████████████████████████████████| 382M/382M [00:08<00:00, 46.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Google Drive Demo (10 sec with gdown already installed)\n",
    "#https://drive.google.com/file/d/1gsni-hMPU5yGkJDyTcv5wV-vl9NCH7g0/view?usp=drive_link\n",
    "#GDRIVE_DATA_URL=\"https://drive.google.com/drive/folders/1-vYT_rbC2B747dNEVjaMKJvjPmJglPXy?usp=drive_link\"\n",
    "GDRIVE_DATA_URL=\"https://drive.google.com/file/d/1gsni-hMPU5yGkJDyTcv5wV-vl9NCH7g0/view?usp=drive_link\"\n",
    "GDRIVE_FILE_ID=\"1gsni-hMPU5yGkJDyTcv5wV-vl9NCH7g0\"\n",
    "! pip install gdown\n",
    "! gdown $GDRIVE_FILE_ID -O lephare_data_gdrive.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GitHub LFS • *(No Demo)*\n",
    "\n",
    "**Pros:**\n",
    "- There could be options for academic institutions to get more free storage/bandwith (TODO look into)\n",
    "\n",
    "**Cons:**\n",
    "- This is a no-go for us, as the current size of our data is 1.36 GB. \n",
    "> Every account using Git Large File Storage receives 1 GiB of free storage and 1 GiB a month of free bandwidth. If the bandwidth and storage quotas are not enough, you can choose to purchase an additional quota for Git LFS.\n",
    "\\- [GitHub LFS Docs](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-storage-and-bandwidth-usage)\n",
    " Even if we stored the zipped version of our data (382 MB) we would be in trouble very quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain GitHub • [View Repo](https://github.com/OliviaLynn/LEPHARE-data)\n",
    "\n",
    "**Pros:**\n",
    "- Versioning, easy to navigate, nested directories, fast download\n",
    "- TODO: Need to read up on details, but this can integrate really nicely with other GitHub repos that read its data (maybe worth making a little demo)\n",
    "\n",
    " **Cons:**\n",
    " - Does this comply with TOS? (TODO look into)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LEPHARE-data'...\n",
      "remote: Enumerating objects: 18763, done.\u001b[K\n",
      "remote: Counting objects: 100% (2376/2376), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1955/1955), done.\u001b[K\n",
      "remote: Total 18763 (delta 403), reused 2375 (delta 403), pack-reused 16387\u001b[K\n",
      "Receiving objects: 100% (18763/18763), 346.51 MiB | 4.94 MiB/s, done.\n",
      "Resolving deltas: 100% (617/617), done.\n",
      "Updating files: 100% (19383/19383), done.\n"
     ]
    }
   ],
   "source": [
    "# GitHub Demo (~1 min)\n",
    "\n",
    "GITHUB_DATA_URL=\"https://github.com/OliviaLynn/LEPHARE-data.git\"\n",
    "! git clone $GITHUB_DATA_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potentially, maybe: NERSC storage? \n",
    "\n",
    "Would have to ask Sam what the general size caps we had in mind for this. The advantage is this would facilitate sharing of templates between other rail template codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 orl  staff  382442106 Mar 20 11:01 lephare_data_gdrive.zip\n",
      "-rw-r--r--  1 orl  staff  305758858 Mar 20 08:38 lephare_data_osf.zip\n",
      "-rw-r--r--@ 1 orl  staff  382442348 Mar 20 10:50 lephare_data_osf_as_zip.zip\n",
      "-rw-r--r--  1 orl  staff  382442280 Mar 20 11:02 lephare_data_zenodo.zip\n"
     ]
    }
   ],
   "source": [
    "# Compare zip sizes\n",
    "# A disclaimer that lephare_data_osf.zip will be smaller because I never uploaded that last sed/STARS dir after realizing it is probably moot.\n",
    "\n",
    "! ls -l *.zip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
